[2024-08-13 17:43:19] [INFO ] [Example0001:29] - Kafka streams word count example
[2024-08-13 17:43:19] [INFO ] [StreamsConfig:372] - StreamsConfig values: 
	acceptable.recovery.lag = 10000
	application.id = word-count-001
	application.server = 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:29092]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.client.supplier = class org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.dsl.store = rocksDB
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.list.key.serde.inner = null
	default.list.key.serde.type = null
	default.list.value.serde.inner = null
	default.list.value.serde.type = null
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	dsl.store.suppliers.class = class org.apache.kafka.streams.state.BuiltInDslStoreSuppliers$RocksDBDslStoreSuppliers
	enable.metrics.push = true
	max.task.idle.ms = 0
	max.warmup.replicas = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	poll.ms = 100
	probing.rebalance.interval.ms = 600000
	processing.guarantee = at_least_once
	rack.aware.assignment.non_overlap_cost = null
	rack.aware.assignment.strategy = none
	rack.aware.assignment.tags = []
	rack.aware.assignment.traffic_cost = null
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	repartition.purge.interval.ms = 30000
	replication.factor = -1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = C:\one-place\kafka-streams\data
	statestore.cache.max.bytes = 0
	task.assignor.class = null
	task.timeout.ms = 300000
	topology.optimization = none
	upgrade.from = null
	window.size.ms = null
	windowed.inner.class.serde = null
	windowstore.changelog.additional.retention.ms = 86400000

[2024-08-13 17:43:20] [INFO ] [StateDirectory:187] - Created new process id: 9235a76c-6623-431b-8920-9911c3c4883a
[2024-08-13 17:43:20] [INFO ] [AdminClientConfig:372] - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

[2024-08-13 17:43:20] [INFO ] [AppInfoParser:124] - Kafka version: 3.8.0
[2024-08-13 17:43:20] [INFO ] [AppInfoParser:125] - Kafka commitId: 771b9576b00ecf5b
[2024-08-13 17:43:20] [INFO ] [AppInfoParser:126] - Kafka startTimeMs: 1723551200775
[2024-08-13 17:43:20] [INFO ] [KafkaStreams:998] - stream-client [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a] Kafka Streams version: 3.8.0
[2024-08-13 17:43:20] [INFO ] [KafkaStreams:999] - stream-client [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a] Kafka Streams commit ID: 771b9576b00ecf5b
[2024-08-13 17:43:20] [INFO ] [StreamThread:383] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] Creating restore consumer client
[2024-08-13 17:43:20] [INFO ] [ConsumerConfig:372] - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[2024-08-13 17:43:20] [INFO ] [KafkaMetricsCollector:269] - initializing Kafka metrics collector
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:124] - Kafka version: 3.8.0
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:125] - Kafka commitId: 771b9576b00ecf5b
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:126] - Kafka startTimeMs: 1723551201014
[2024-08-13 17:43:21] [INFO ] [StreamThread:104] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] Creating thread producer client
[2024-08-13 17:43:21] [INFO ] [ProducerConfig:372] - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-producer
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[2024-08-13 17:43:21] [INFO ] [KafkaMetricsCollector:269] - initializing Kafka metrics collector
[2024-08-13 17:43:21] [INFO ] [KafkaProducer:621] - [Producer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-producer] Instantiated an idempotent producer.
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:124] - Kafka version: 3.8.0
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:125] - Kafka commitId: 771b9576b00ecf5b
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:126] - Kafka startTimeMs: 1723551201157
[2024-08-13 17:43:21] [INFO ] [DefaultStateUpdater:144] - state-updater [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StateUpdater-1] State updater thread started
[2024-08-13 17:43:21] [INFO ] [StreamThread:461] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] Creating consumer client
[2024-08-13 17:43:21] [INFO ] [ConsumerConfig:372] - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = word-count-001
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[2024-08-13 17:43:21] [INFO ] [KafkaMetricsCollector:269] - initializing Kafka metrics collector
[2024-08-13 17:43:21] [INFO ] [AssignorConfiguration:144] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] Cooperative rebalancing protocol is enabled now
[2024-08-13 17:43:21] [INFO ] [ConsumerConfig:381] - These configurations '[replication.factor, probing.rebalance.interval.ms, max.warmup.replicas, acceptable.recovery.lag, task.assignor.class, rack.aware.assignment.non_overlap_cost, application.server, rack.aware.assignment.strategy, rack.aware.assignment.traffic_cost, windowstore.changelog.additional.retention.ms, num.standby.replicas, upgrade.from, rack.aware.assignment.tags, application.id]' were supplied but are not used yet.
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:124] - Kafka version: 3.8.0
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:125] - Kafka commitId: 771b9576b00ecf5b
[2024-08-13 17:43:21] [INFO ] [AppInfoParser:126] - Kafka startTimeMs: 1723551201352
[2024-08-13 17:43:21] [INFO ] [KafkaStreams:346] - stream-client [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a] State transition from CREATED to REBALANCING
[2024-08-13 17:43:21] [INFO ] [KafkaStreams:1411] - stream-client [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a] Started 1 stream threads
[2024-08-13 17:43:21] [INFO ] [StreamThread:663] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] Starting
[2024-08-13 17:43:21] [INFO ] [StreamThread:250] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] State transition from CREATED to STARTING
[2024-08-13 17:43:21] [INFO ] [LegacyKafkaConsumer:476] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Subscribed to topic(s): input-topic-001
[2024-08-13 17:43:21] [INFO ] [StreamThread:1074] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
[2024-08-13 17:43:21] [INFO ] [Metadata:364] - [Producer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-producer] Cluster ID: 5i33DAZvQZSJkXWY3utrDg
[2024-08-13 17:43:21] [INFO ] [TransactionManager:502] - [Producer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-producer] ProducerId set to 41 with epoch 0
[2024-08-13 17:43:21] [INFO ] [Metadata:364] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Cluster ID: 5i33DAZvQZSJkXWY3utrDg
[2024-08-13 17:43:21] [INFO ] [ConsumerCoordinator:936] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Discovered group coordinator localhost:29092 (id: 2147483646 rack: null)
[2024-08-13 17:43:21] [INFO ] [ConsumerCoordinator:604] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] (Re-)joining group
[2024-08-13 17:43:21] [INFO ] [ConsumerCoordinator:1102] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Request joining group due to: need to re-join with the given member-id: word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer-99691984-e3ae-4a40-9700-eef67a65353f
[2024-08-13 17:43:21] [INFO ] [ConsumerCoordinator:604] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] (Re-)joining group
[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:665] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Successfully joined group with generation Generation{generationId=20, memberId='word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer-99691984-e3ae-4a40-9700-eef67a65353f', protocol='stream'}
[2024-08-13 17:43:56] [INFO ] [RepartitionTopics:75] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] Skipping the repartition topic validation since there are no repartition topics.
[2024-08-13 17:43:56] [INFO ] [StreamsPartitionAssignor:778] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] 1 client nodes and 1 consumers participating in this rebalance: 
9235a76c-6623-431b-8920-9911c3c4883a: [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer-99691984-e3ae-4a40-9700-eef67a65353f].
[2024-08-13 17:43:56] [INFO ] [StreamsPartitionAssignor:789] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] Assigning stateful tasks: []
and stateless tasks: [0_2, 0_1, 0_0]
[2024-08-13 17:43:56] [INFO ] [AssignorConfiguration:259] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] No custom task assignors found, defaulting to internal task assignment with internal.task.assignor.class
[2024-08-13 17:43:56] [INFO ] [HighAvailabilityTaskAssignor:107] - Decided on assignment: {9235a76c-6623-431b-8920-9911c3c4883a=[activeTasks: ([0_0, 0_1, 0_2]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([]) clientTags: ([]) capacity: 1 assigned: 3]} with no followup probing rebalance.
[2024-08-13 17:43:56] [INFO ] [StreamsPartitionAssignor:850] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] Assigned 3 total tasks including 0 stateful tasks to 1 client nodes.
[2024-08-13 17:43:56] [INFO ] [StreamsPartitionAssignor:854] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] Assignment of tasks to nodes: 9235a76c-6623-431b-8920-9911c3c4883a=[activeTasks: ([0_0, 0_1, 0_2]) standbyTasks: ([])]
[2024-08-13 17:43:56] [INFO ] [StreamsPartitionAssignor:1081] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] Client 9235a76c-6623-431b-8920-9911c3c4883a per-consumer assignment:
	prev owned active {}
	prev owned standby {word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer-99691984-e3ae-4a40-9700-eef67a65353f=[]}
	assigned active {word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer-99691984-e3ae-4a40-9700-eef67a65353f=[0_2, 0_1, 0_0]}
	revoking active {}
	assigned standby {}

[2024-08-13 17:43:56] [INFO ] [StreamsPartitionAssignor:1100] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required.
[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:663] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Finished assignment for group at generation 20: {word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer-99691984-e3ae-4a40-9700-eef67a65353f=Assignment(partitions=[input-topic-001-0, input-topic-001-1, input-topic-001-2], userDataSize=76)}
[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:842] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Successfully synced group in generation Generation{generationId=20, memberId='word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer-99691984-e3ae-4a40-9700-eef67a65353f', protocol='stream'}
[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:386] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Updating assignment with
	Assigned partitions:                       [input-topic-001-0, input-topic-001-1, input-topic-001-2]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [input-topic-001-0, input-topic-001-1, input-topic-001-2]
	Revoked partitions (owned - assigned):     []

[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:323] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Notifying assignor about the new Assignment(partitions=[input-topic-001-0, input-topic-001-1, input-topic-001-2], userDataSize=76)
[2024-08-13 17:43:56] [INFO ] [StreamsPartitionAssignor:1581] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule.
[2024-08-13 17:43:56] [INFO ] [TaskManager:335] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] Handle new assignment with:
	New active tasks: [0_2, 0_1, 0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
[2024-08-13 17:43:56] [INFO ] [ConsumerRebalanceListenerInvoker:57] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Adding newly assigned partitions: input-topic-001-0, input-topic-001-1, input-topic-001-2
[2024-08-13 17:43:56] [INFO ] [StreamThread:250] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:1506] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Found no committed offset for partition input-topic-001-1
[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:1506] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Found no committed offset for partition input-topic-001-0
[2024-08-13 17:43:56] [INFO ] [ConsumerUtils:201] - Setting offset for partition input-topic-001-2 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:29092 (id: 1 rack: null)], epoch=0}}
[2024-08-13 17:43:56] [INFO ] [SubscriptionState:398] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Resetting offset for partition input-topic-001-1 to position FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:29092 (id: 1 rack: null)], epoch=0}}.
[2024-08-13 17:43:56] [INFO ] [SubscriptionState:398] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Resetting offset for partition input-topic-001-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:29092 (id: 1 rack: null)], epoch=0}}.
[2024-08-13 17:43:56] [INFO ] [StreamTask:267] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] task [0_0] Initialized
[2024-08-13 17:43:56] [INFO ] [StreamTask:267] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] task [0_1] Initialized
[2024-08-13 17:43:56] [INFO ] [DefaultStateUpdater:488] - state-updater [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StateUpdater-1] Stateless active task 0_0 was added to the restored tasks of the state updater
[2024-08-13 17:43:56] [INFO ] [StreamTask:267] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] task [0_2] Initialized
[2024-08-13 17:43:56] [INFO ] [DefaultStateUpdater:488] - state-updater [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StateUpdater-1] Stateless active task 0_1 was added to the restored tasks of the state updater
[2024-08-13 17:43:56] [INFO ] [DefaultStateUpdater:488] - state-updater [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StateUpdater-1] Stateless active task 0_2 was added to the restored tasks of the state updater
[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:1506] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Found no committed offset for partition input-topic-001-0
[2024-08-13 17:43:56] [INFO ] [StreamTask:295] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] task [0_0] Restored and ready to run
[2024-08-13 17:43:56] [INFO ] [ConsumerCoordinator:1506] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Found no committed offset for partition input-topic-001-1
[2024-08-13 17:43:56] [INFO ] [StreamTask:295] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] task [0_1] Restored and ready to run
[2024-08-13 17:43:56] [INFO ] [StreamTask:295] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] task [0_2] Restored and ready to run
[2024-08-13 17:43:56] [INFO ] [StreamThread:250] - stream-thread [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
[2024-08-13 17:43:56] [INFO ] [KafkaStreams:346] - stream-client [word-count-001-9235a76c-6623-431b-8920-9911c3c4883a] State transition from REBALANCING to RUNNING
[2024-08-13 17:43:56] [INFO ] [Example0001:78] - key=key4, value=Rita Skeeter
[2024-08-13 17:43:56] [INFO ] [LegacyKafkaConsumer:1047] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Requesting the log end offset for input-topic-001-1 in order to compute lag
[2024-08-13 17:43:56] [INFO ] [LegacyKafkaConsumer:1047] - [Consumer clientId=word-count-001-9235a76c-6623-431b-8920-9911c3c4883a-StreamThread-1-consumer, groupId=word-count-001] Requesting the log end offset for input-topic-001-2 in order to compute lag
[2024-08-13 17:43:57] [INFO ] [Example0001:78] - key=key3, value=Cassius Warrington
[2024-08-13 17:43:57] [INFO ] [Example0001:78] - key=key5, value=Norbert
[2024-08-13 17:43:57] [INFO ] [Example0001:78] - key=key1, value=Oliver Wood
[2024-08-13 17:43:57] [INFO ] [Example0001:78] - key=key2, value=Kingsley Shacklebolt
[2024-08-13 17:43:57] [INFO ] [Example0001:78] - key=key3, value=Fenrir Greyback
[2024-08-13 17:43:57] [INFO ] [Example0001:78] - key=key1, value=Cadmus Peverell
[2024-08-13 17:43:57] [INFO ] [Example0001:78] - key=key2, value=Fred Weasley
[2024-08-13 17:43:57] [INFO ] [Example0001:78] - key=key4, value=Kennilworthy Whisp
[2024-08-13 17:44:07] [INFO ] [Example0001:78] - key=key5, value=Nymphadora Tonks
